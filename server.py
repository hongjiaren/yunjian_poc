# æ¨¡å—ä¸€ ï¼šrpaå®Œæˆï¼ˆç›®å‰ä¸ç”¨åšï¼‰
# æ¨¡å—äºŒ ï¼šè°ƒç”¨æ™ºèƒ½ä½“çš„ç”Ÿæˆç»“æœ
# æ¨¡å—ä¸‰ ï¼šè°ƒç”¨vlå¤§æ¨¡å‹ï¼ˆgpt4o+qwenvlï¼‰

import os
import base64
from mimetypes import guess_type
import requests
from docx import Document
from docx.oxml.ns import qn
from lxml import etree
import re
from tool import extract_images_and_find_target
from dashscope import MultiModalConversation
from http import HTTPStatus
from openai import OpenAI
import json
from prompt import HARDWARE_ANALYSIS_PROMPT


# ================== é…ç½®åŒºåŸŸ ==================
DOC_PATH = "file/0507ã€è¾“å‡º-ç”Ÿäº§æµ‹è¯•æ–¹æ¡ˆ-æ¡ˆä¾‹-æ¨èã€‘äº§å“ç”Ÿäº§æµ‹è¯•æ–¹æ¡ˆ.docx"
TARGET_TITLE = "æ•´æœºæµ‹è¯•ç³»ç»Ÿæ¡†å›¾"

# é…ç½®é¡¹ï¼ˆè¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ Azure é…ç½®ï¼‰
OPENAI_API_KEY = "ä½ çš„AzureOpenAIAPIKey"
RESOURCE_NAME = "ä½ çš„èµ„æºåç§°"   # å¦‚ my-openai-resource
DEPLOYMENT_NAME = "ä½ çš„éƒ¨ç½²åç§°" # å¦‚ gpt-4o-vision
API_VERSION = "2024-02-15-preview"




def encode_image(image_data):
    """å°†å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸º base64"""
    return base64.b64encode(image_data).decode("utf-8")


def image_to_data_url(image_data, mime_type=None):
    """
    å°†å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸º data URI æ ¼å¼ã€‚
    å¯è‡ªåŠ¨è¯†åˆ«ç±»å‹æˆ–æ‰‹åŠ¨æŒ‡å®šï¼ˆå¦‚ 'image/png'ï¼‰
    """
    if not mime_type:
        # è‡ªåŠ¨çŒœæµ‹ MIME ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ jpeg
        mime_type = guess_type('image.jpg')[0] or 'image/jpeg'
    encoded = encode_image(image_data)
    return f"data:{mime_type};base64,{encoded}"


def analyze_with_azure_gpt4v(image_data, prompt="è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚", detail_level="auto"):

    print("ğŸ§  æ­£åœ¨è°ƒç”¨ Azure ä¸Šçš„ GPT-4 Vision è¿›è¡Œå›¾åƒåˆ†æ...")

    # å°†å›¾ç‰‡è½¬ä¸º data:image;base64,...
    image_url = image_to_data_url(image_data)

    # è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "api-key": OPENAI_API_KEY
    }

    # è¯·æ±‚ä½“
    payload = {
        "model": DEPLOYMENT_NAME,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_url,
                            "detail": detail_level
                        }
                    }
                ]
            }
        ],
        "max_tokens": 500,
        "stream": False
    }

    # æ„å»ºè¯·æ±‚ URL
    endpoint = f"https://{RESOURCE_NAME}.openai.azure.com/openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}"

    # å‘é€è¯·æ±‚
    response = requests.post(endpoint, headers=headers, json=payload)

    # å¤„ç†å“åº”
    if response.status_code == 200:
        result = response.json()
        if 'choices' in result and len(result['choices']) > 0:
            return result["choices"][0]["message"]["content"]
        else:
            return "âŒ å“åº”ä¸­æ²¡æœ‰å†…å®¹ï¼š" + str(result)
    else:
        return f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}ï¼š{response.text}"


def analyze_with_qwen_vl(image_data, prompt):

    # Set OpenAI's API key and API base to use vLLM's API server.
    print("ğŸ§  æ­£åœ¨è°ƒç”¨æœ¬åœ°çš„qwen2.5vlè¿›è¡Œå›¾åƒåˆ†æ...")

    # å°†å›¾ç‰‡è½¬ä¸º data:image;base64,...
    image_url = image_to_data_url(image_data)
    openai_api_key = "EMPTY"
    openai_api_base = "http://223.109.239.12:8008/v1"

    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

    chat_response = client.chat.completions.create(
        model="/dataAll/liweier/Qwen2.5-VL-72B-Instruct-bnb-4bit",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_url
                        },
                    },
                    {"type": "text", "text": prompt},
                ],
            },
        ],
    )

    # ç›´æ¥è·å–æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬å†…å®¹
    response_content = chat_response.choices[0].message.content
    print("ğŸ“ æ¨¡å‹è¾“å‡ºå†…å®¹å¦‚ä¸‹ï¼š")
    print(response_content)

    try:
        cleaned_response_content = response_content.strip().replace('```json', '').replace('\n', '').replace('```', '')
        parsed_response = json.loads(cleaned_response_content)
        print(parsed_response)
        return parsed_response
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        return None


# # 
# æµ‹è¯•è°ƒç”¨
# ================================================

if __name__ == "__main__":
    # Step 1: æŸ¥æ‰¾æ ‡é¢˜å¹¶æå–å›¾ç‰‡ï¼ˆè¿”å›çš„æ˜¯å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°æ®ï¼‰
    image_data = extract_images_and_find_target(DOC_PATH, TARGET_TITLE)

    if image_data:
        # Step 2: ä½¿ç”¨ GPT-4o åˆ†æå›¾ç‰‡ï¼ˆç›´æ¥ä¼ å…¥å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ï¼‰
        # gpt4o_result = analyze_with_azure_gpt4v(image_data)
        # print("\nğŸŸ¢ã€GPT-4oã€‘åˆ†æç»“æœï¼š\n")
        # print(gpt4o_result)

        # Step 3: ä½¿ç”¨ Qwen2.5-VL åˆ†æå›¾ç‰‡ï¼ˆç›´æ¥ä¼ å…¥å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ï¼‰
        prompt = HARDWARE_ANALYSIS_PROMPT
        qwen_result = analyze_with_qwen_vl(image_data, prompt)
        print("\nğŸ”µã€Qwen2.5-VLã€‘åˆ†æç»“æœå¤„ç†åï¼š\n")
        print(qwen_result)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œæ— æ³•è¿›è¡Œè§†è§‰åˆ†æã€‚")